<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.2.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="//cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.1/css/all.min.css">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/npm/animate.css@3.1.1/animate.min.css">

<script class="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Muse","version":"8.0.2","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":false,"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}};
  </script>

  <meta name="description" content="Scrapy 功能非常强大，爬取效率高，相关扩展组件多，可配置和可扩展程度非常高，几乎可以应对所有反爬网站，是目前Python中使用最广泛的爬虫框架 官方文档 https:&#x2F;&#x2F;doc.scrapy.org&#x2F;en&#x2F;latest&#x2F;index.html Scrapy创建项目1scrapy startproject tutorial   scrapy.cfg：Scrapy 项目的配置文件，定义了项目配置文">
<meta property="og:type" content="article">
<meta property="og:title" content="Python3网络爬虫开发实战-Scrapy框架使用">
<meta property="og:url" content="http://example.com/2022/04/19/Python3%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%E5%BC%80%E5%8F%91%E5%AE%9E%E6%88%98-Scrapy%E6%A1%86%E6%9E%B6%E4%BD%BF%E7%94%A8/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="Scrapy 功能非常强大，爬取效率高，相关扩展组件多，可配置和可扩展程度非常高，几乎可以应对所有反爬网站，是目前Python中使用最广泛的爬虫框架 官方文档 https:&#x2F;&#x2F;doc.scrapy.org&#x2F;en&#x2F;latest&#x2F;index.html Scrapy创建项目1scrapy startproject tutorial   scrapy.cfg：Scrapy 项目的配置文件，定义了项目配置文">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/Python3%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%E5%BC%80%E5%8F%91%E5%AE%9E%E6%88%98-Scrapy%E6%A1%86%E6%9E%B6%E4%BD%BF%E7%94%A8/WeChatf2beb120c54835739c5d7653fc0fc630.png">
<meta property="og:image" content="http://example.com/Python3%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%E5%BC%80%E5%8F%91%E5%AE%9E%E6%88%98-Scrapy%E6%A1%86%E6%9E%B6%E4%BD%BF%E7%94%A8/WeChatb6f074004233d523d5e359e02e4ffb4e.png">
<meta property="og:image" content="http://example.com/2022/04/19/Python3%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%E5%BC%80%E5%8F%91%E5%AE%9E%E6%88%98-Scrapy%E6%A1%86%E6%9E%B6%E4%BD%BF%E7%94%A8/Python3网络爬虫开发实战-Scrapy框架使用/WeChatae25474d863d90da3fe12abbc4a54da0.png">
<meta property="article:published_time" content="2022-04-19T14:00:23.000Z">
<meta property="article:modified_time" content="2022-04-24T01:30:18.911Z">
<meta property="article:author" content="daxun">
<meta property="article:tag" content="Python">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/Python3%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%E5%BC%80%E5%8F%91%E5%AE%9E%E6%88%98-Scrapy%E6%A1%86%E6%9E%B6%E4%BD%BF%E7%94%A8/WeChatf2beb120c54835739c5d7653fc0fc630.png">


<link rel="canonical" href="http://example.com/2022/04/19/Python3%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%E5%BC%80%E5%8F%91%E5%AE%9E%E6%88%98-Scrapy%E6%A1%86%E6%9E%B6%E4%BD%BF%E7%94%A8/">


<script class="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>Python3网络爬虫开发实战-Scrapy框架使用 | Hexo</title>
  






  <noscript>
  <style>
  body { margin-top: 2rem; }

  .use-motion .menu-item,
  .use-motion .sidebar,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header {
    visibility: visible;
  }

  .use-motion .header,
  .use-motion .site-brand-container .toggle,
  .use-motion .footer { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle,
  .use-motion .custom-logo-image {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line {
    transform: scaleX(1);
  }

  .search-pop-overlay, .sidebar-nav { display: none; }
  .sidebar-panel { display: block; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">Hexo</h1>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">简单记录下</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</div>
        
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <section class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-4"><a class="nav-link" href="#Scrapy"><span class="nav-number">1.</span> <span class="nav-text">Scrapy</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%88%9B%E5%BB%BA%E9%A1%B9%E7%9B%AE"><span class="nav-number">1.1.</span> <span class="nav-text">创建项目</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%88%9B%E5%BB%BA-Spider"><span class="nav-number">1.2.</span> <span class="nav-text">创建 Spider</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%88%9B%E5%BB%BA-Item"><span class="nav-number">1.3.</span> <span class="nav-text">创建 Item</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E8%A7%A3%E6%9E%90-Response"><span class="nav-number">1.4.</span> <span class="nav-text">解析 Response</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E4%BD%BF%E7%94%A8-Item"><span class="nav-number">1.5.</span> <span class="nav-text">使用 Item</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%90%8E%E7%BB%AD-Request"><span class="nav-number">1.6.</span> <span class="nav-text">后续 Request</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E8%BF%90%E8%A1%8C"><span class="nav-number">1.7.</span> <span class="nav-text">运行</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E4%BF%9D%E5%AD%98%E5%88%B0%E6%96%87%E4%BB%B6"><span class="nav-number">1.8.</span> <span class="nav-text">保存到文件</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E4%BD%BF%E7%94%A8-Item-Pipeline"><span class="nav-number">1.9.</span> <span class="nav-text">使用 Item Pipeline</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#13-3-Selecotr-%E7%94%A8%E6%B3%95"><span class="nav-number">2.</span> <span class="nav-text">13.3 Selecotr 用法</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E7%9B%B4%E6%8E%A5%E4%BD%BF%E7%94%A8"><span class="nav-number">2.1.</span> <span class="nav-text">直接使用</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Scrapy-shell"><span class="nav-number">2.2.</span> <span class="nav-text">Scrapy shell</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#XPath-%E9%80%89%E6%8B%A9%E5%99%A8"><span class="nav-number">2.3.</span> <span class="nav-text">XPath 选择器</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#CSS-%E9%80%89%E6%8B%A9%E5%99%A8"><span class="nav-number">2.4.</span> <span class="nav-text">CSS 选择器</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%AD%A3%E5%88%99%E5%8C%B9%E9%85%8D"><span class="nav-number">2.5.</span> <span class="nav-text">正则匹配</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#13-4-Spider-%E7%94%A8%E6%B3%95"><span class="nav-number">3.</span> <span class="nav-text">13.4 Spider 用法</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Spider-%E7%B1%BB"><span class="nav-number">3.1.</span> <span class="nav-text">Spider 类</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#13-5-Downloader-Middleware"><span class="nav-number">4.</span> <span class="nav-text">13.5 Downloader Middleware</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%A0%B8%E5%BF%83%E6%96%B9%E6%B3%95"><span class="nav-number">4.1.</span> <span class="nav-text">核心方法</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%AE%9E%E6%88%98"><span class="nav-number">4.2.</span> <span class="nav-text">实战</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#13-6-Spider-Middleware"><span class="nav-number">5.</span> <span class="nav-text">13.6 Spider Middleware</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#13-6-Item-Pipeline"><span class="nav-number">6.</span> <span class="nav-text">13.6 Item Pipeline</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%88%AC%E5%8F%96-360%E6%91%84%E5%BD%B1%E7%BE%8E%E5%9B%BE"><span class="nav-number">7.</span> <span class="nav-text">爬取 360摄影美图</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%9E%84%E9%80%A0%E8%AF%B7%E6%B1%82"><span class="nav-number">7.1.</span> <span class="nav-text">构造请求</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#MongoDB%E5%AD%98%E5%82%A8"><span class="nav-number">7.2.</span> <span class="nav-text">MongoDB存储</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#MySQL-%E5%AD%98%E5%82%A8"><span class="nav-number">7.3.</span> <span class="nav-text">MySQL 存储</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Image-Pipeline"><span class="nav-number">7.4.</span> <span class="nav-text">Image Pipeline</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#13-8-Scrapy-%E5%AF%B9%E6%8E%A5-Selenium"><span class="nav-number">8.</span> <span class="nav-text">13.8 Scrapy 对接 Selenium</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%AF%B9%E6%8E%A5-Selenium"><span class="nav-number">8.1.</span> <span class="nav-text">对接 Selenium</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E8%A7%A3%E6%9E%90%E9%A1%B5%E9%9D%A2"><span class="nav-number">8.2.</span> <span class="nav-text">解析页面</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#13-9-Scrapy-%E5%AF%B9%E6%8E%A5-Splash"><span class="nav-number">9.</span> <span class="nav-text">13.9 Scrapy 对接 Splash</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#13-10-%E9%80%9A%E7%94%A8%E7%88%AC%E8%99%AB"><span class="nav-number">10.</span> <span class="nav-text">13.10 通用爬虫</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#CrawSpider"><span class="nav-number">10.1.</span> <span class="nav-text">CrawSpider</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#Rule"><span class="nav-number">10.1.1.</span> <span class="nav-text">Rule</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#Item-Loader"><span class="nav-number">10.1.2.</span> <span class="nav-text">Item Loader</span></a></li></ol></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E7%88%AC%E5%8F%96%E4%B8%AD%E5%8D%8E%E7%BD%91%E7%A7%91%E6%8A%80%E7%B1%BB%E6%96%B0%E9%97%BB"><span class="nav-number">10.2.</span> <span class="nav-text">爬取中华网科技类新闻</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#%E6%96%B0%E5%BB%BA%E9%A1%B9%E7%9B%AE"><span class="nav-number">10.2.1.</span> <span class="nav-text">新建项目</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#%E5%AE%9A%E4%B9%89-Rule"><span class="nav-number">10.2.2.</span> <span class="nav-text">定义 Rule</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#%E8%A7%A3%E6%9E%90%E9%A1%B5%E9%9D%A2-1"><span class="nav-number">10.2.3.</span> <span class="nav-text">解析页面</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#%E9%80%9A%E7%94%A8%E9%85%8D%E7%BD%AE%E6%8A%BD%E5%8F%96"><span class="nav-number">10.2.4.</span> <span class="nav-text">通用配置抽取</span></a></li></ol></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#13-11-Scrapyt"><span class="nav-number">11.</span> <span class="nav-text">13.11 Scrapyt</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#GET-%E8%AF%B7%E6%B1%82"><span class="nav-number">11.1.</span> <span class="nav-text">GET 请求</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#POST-%E8%AF%B7%E6%B1%82"><span class="nav-number">11.2.</span> <span class="nav-text">POST 请求</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Scrapy%E5%AF%B9%E6%8E%A5Docker"><span class="nav-number">12.</span> <span class="nav-text">Scrapy对接Docker</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%88%9B%E5%BB%BADockerfile"><span class="nav-number">12.1.</span> <span class="nav-text">创建Dockerfile</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%9E%84%E5%BB%BA%E9%95%9C%E5%83%8F"><span class="nav-number">12.2.</span> <span class="nav-text">构建镜像</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E8%BF%90%E8%A1%8C-1"><span class="nav-number">12.3.</span> <span class="nav-text">运行</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%8E%A8%E9%80%81%E8%87%B3-Docker-Hub"><span class="nav-number">12.4.</span> <span class="nav-text">推送至 Docker Hub</span></a></li></ol></li></ol></div>
        </section>
        <!--/noindex-->

        <section class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">daxun</p>
  <div class="site-description" itemprop="description">简单记录下</div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives">
        
          <span class="site-state-item-count">76</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories">
          
        <span class="site-state-item-count">8</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags">
          
        <span class="site-state-item-count">6</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>



        </section>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">
      

      

  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/04/19/Python3%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%E5%BC%80%E5%8F%91%E5%AE%9E%E6%88%98-Scrapy%E6%A1%86%E6%9E%B6%E4%BD%BF%E7%94%A8/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="daxun">
      <meta itemprop="description" content="简单记录下">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Python3网络爬虫开发实战-Scrapy框架使用
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-04-19 22:00:23" itemprop="dateCreated datePublished" datetime="2022-04-19T22:00:23+08:00">2022-04-19</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">更新于</span>
        <time title="修改时间：2022-04-24 09:30:18" itemprop="dateModified" datetime="2022-04-24T09:30:18+08:00">2022-04-24</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Python/" itemprop="url" rel="index"><span itemprop="name">Python</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <p>Scrapy 功能非常强大，爬取效率高，相关扩展组件多，可配置和可扩展程度非常高，几乎可以应对所有反爬网站，是目前Python中使用最广泛的爬虫框架</p>
<p>官方文档 <a target="_blank" rel="noopener" href="https://doc.scrapy.org/en/latest/index.html">https://doc.scrapy.org/en/latest/index.html</a></p>
<h4 id="Scrapy"><a href="#Scrapy" class="headerlink" title="Scrapy"></a>Scrapy</h4><h5 id="创建项目"><a href="#创建项目" class="headerlink" title="创建项目"></a>创建项目</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy startproject tutorial</span><br></pre></td></tr></table></figure>

<p><img src="/Python3%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%E5%BC%80%E5%8F%91%E5%AE%9E%E6%88%98-Scrapy%E6%A1%86%E6%9E%B6%E4%BD%BF%E7%94%A8/WeChatf2beb120c54835739c5d7653fc0fc630.png" alt="WeChatf2beb120c54835739c5d7653fc0fc630"></p>
<p>scrapy.cfg：Scrapy 项目的配置文件，定义了项目配置文件路径、部署相关信息等内容，Scrapy 部署时的配置文件</p>
<p>items.py：定义 Item 数据结构，所有的 Item 的定义都可以放这里，定义爬取的数据结构</p>
<p>pipelines.py：定义 Item Pipeline 的实现，所有 Item Pipeline 的实现都可以放这里，定义数据管道</p>
<p>settings.py：定义项目的全局配置，配置文件</p>
<p>middlewares.py：定义 Spider Middlewares 和 Downloader Middlewares 的实现，爬取时的中间件</p>
<p>spiders：其内包含一个个 Spider 的实现，每个 Spider 都有一个文件，放置 Spiders 的文件夹</p>
<h5 id="创建-Spider"><a href="#创建-Spider" class="headerlink" title="创建 Spider"></a>创建 Spider</h5><p>Spider 是自己定义的类，Scrapy 用它来从网页抓取内容，并解析抓取的结果，不过这个类必须继承 Scrapy 提供的 Spider 类 scrapy.Spider，还要定义 Spider 的名称和起始请求，以及怎样处理爬取后的结果的方法</p>
<p>命令行创建一个 Spider，生成 Quotes 这个 Spider</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd tutorial</span><br><span class="line">scrapy genspider quotes quotes.toscrape.com</span><br></pre></td></tr></table></figure>

<p>执行 genspider 命令，第一个参数是 Spider 名称，第二个参数是网站域名</p>
<p>执行完毕后，spiders 文件夹中多了一个 quotes.py</p>
<p>内容：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">QuotesSpider</span>(<span class="params">scrapy.Spider</span>):</span></span><br><span class="line">    name = <span class="string">&#x27;quotes&#x27;</span></span><br><span class="line">    allowed_domains = [<span class="string">&#x27;quotes.toscrape.com&#x27;</span>]</span><br><span class="line">    start_urls = [<span class="string">&#x27;http://quotes.toscrape.com/&#x27;</span>]</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span>(<span class="params">self, response</span>):</span></span><br><span class="line">        <span class="keyword">pass</span></span><br></pre></td></tr></table></figure>

<p>name 每个项目唯一的名字，区分不同的Spider</p>
<p>allowed_domains 允许爬取的域名，如果初始或后续的请求链接不在这个域名下的，则请求链接会被过滤</p>
<p>start_urls，包含了Spider在启动时爬取的url列表，初始请求是由它来定义的</p>
<p>parse 是 Spider 的一个方法，默认情况下，被调用时 start_urls 里面的链接构成的请求完成下载执行后，返回的响应会作为唯一的参数传递给这个函数，该方法解析返回的响应、提取数据或者进一步生成要处理的请求</p>
<h5 id="创建-Item"><a href="#创建-Item" class="headerlink" title="创建 Item"></a>创建 Item</h5><p>Item 是保存爬取数据的容器，使用方法和字典类似</p>
<p>创建 Item 需要继承 scrapy.Item 类，并且定义类型为 scrapy.Field 的字段</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">QuoteItem</span>(<span class="params">scrapy.Item</span>):</span></span><br><span class="line">    <span class="comment"># define the fields for your item here like:</span></span><br><span class="line">    text = scrapy.Field()</span><br><span class="line">    author = scrapy.Field()</span><br><span class="line">    tags = scrapy.Field()</span><br><span class="line">    <span class="keyword">pass</span></span><br></pre></td></tr></table></figure>

<h5 id="解析-Response"><a href="#解析-Response" class="headerlink" title="解析 Response"></a>解析 Response</h5><p>parse() 方法的参数 response 是 start_urls 里面的链接爬取后的结果，所以在 parse() 方法中，我们可以直接对 response 变量包含的内容进行解析</p>
<p><img src="/Python3%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%E5%BC%80%E5%8F%91%E5%AE%9E%E6%88%98-Scrapy%E6%A1%86%E6%9E%B6%E4%BD%BF%E7%94%A8/WeChatb6f074004233d523d5e359e02e4ffb4e.png" alt="WeChatb6f074004233d523d5e359e02e4ffb4e"></p>
<p>网页结构中每一页都有多个 class 为 quote 的区块，每个区块都包含 text、author、tags，需要先找出所有的 quote，然后提取每一个 quote 中的内容</p>
<p>提取方式可以是 CSS 选择器或XPath 选择器</p>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">&quot;quote&quot;</span> <span class="attr">itemscope</span>=<span class="string">&quot;&quot;</span> <span class="attr">itemtype</span>=<span class="string">&quot;http://schema.org/CreativeWork&quot;</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">span</span> <span class="attr">class</span>=<span class="string">&quot;text&quot;</span> <span class="attr">itemprop</span>=<span class="string">&quot;text&quot;</span>&gt;</span>“The world as we have ....”<span class="tag">&lt;/<span class="name">span</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">span</span>&gt;</span>by <span class="tag">&lt;<span class="name">small</span> <span class="attr">class</span>=<span class="string">&quot;author&quot;</span> <span class="attr">itemprop</span>=<span class="string">&quot;author&quot;</span>&gt;</span>Albert Einstein<span class="tag">&lt;/<span class="name">small</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">&quot;/author/Albert-Einstein&quot;</span>&gt;</span>(about)<span class="tag">&lt;/<span class="name">a</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">span</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">&quot;tags&quot;</span>&gt;</span></span><br><span class="line">        Tags:</span><br><span class="line">        <span class="tag">&lt;<span class="name">meta</span> <span class="attr">class</span>=<span class="string">&quot;keywords&quot;</span> <span class="attr">itemprop</span>=<span class="string">&quot;keywords&quot;</span> <span class="attr">content</span>=<span class="string">&quot;change,deep-thoughts,thinking,world&quot;</span>&gt;</span> </span><br><span class="line">        <span class="tag">&lt;<span class="name">a</span> <span class="attr">class</span>=<span class="string">&quot;tag&quot;</span> <span class="attr">href</span>=<span class="string">&quot;/tag/change/page/1/&quot;</span>&gt;</span>change<span class="tag">&lt;/<span class="name">a</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">a</span> <span class="attr">class</span>=<span class="string">&quot;tag&quot;</span> <span class="attr">href</span>=<span class="string">&quot;/tag/deep-thoughts/page/1/&quot;</span>&gt;</span>deep-thoughts<span class="tag">&lt;/<span class="name">a</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">a</span> <span class="attr">class</span>=<span class="string">&quot;tag&quot;</span> <span class="attr">href</span>=<span class="string">&quot;/tag/thinking/page/1/&quot;</span>&gt;</span>thinking<span class="tag">&lt;/<span class="name">a</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">a</span> <span class="attr">class</span>=<span class="string">&quot;tag&quot;</span> <span class="attr">href</span>=<span class="string">&quot;/tag/world/page/1/&quot;</span>&gt;</span>world<span class="tag">&lt;/<span class="name">a</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse</span>(<span class="params">self, response</span>):</span></span><br><span class="line">    quotes = response.css(<span class="string">&#x27;.quote&#x27;</span>)</span><br><span class="line">    <span class="keyword">for</span> quote <span class="keyword">in</span> quotes:</span><br><span class="line">        text = quote.css(<span class="string">&#x27;.text::text&#x27;</span>).extract_first()</span><br><span class="line">        autnor = quote.css(<span class="string">&#x27;.author::text&#x27;</span>).extract_first()</span><br><span class="line">        tags = quote.css(<span class="string">&#x27;.tags .tag::text&#x27;</span>).extract()</span><br></pre></td></tr></table></figure>

<p>先利用选择器选取所有的 quote，并将其赋值为 quotes 变量，然后利用 for 循环对每个 quote 遍历，解析每个 quote 内容</p>
<p>对 text 来说，它的 class 为 text，所以用 .text 选择器来选取，这个结果实际上是整个带有标签的节点，要获取正文内容，可以加 ::text 来获取，这时的结果是长度为 1 的列表，所以要用 extract_first() 方法来获取第一个元素；对 tags 来说，由于我们要获取所有的标签，所以用 extract() 方法获取整个列表即可</p>
<h5 id="使用-Item"><a href="#使用-Item" class="headerlink" title="使用 Item"></a>使用 Item</h5><p>解析结果赋值 Item 的每一个字段</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tutorial.items <span class="keyword">import</span> QuoteItem</span><br><span class="line">...</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">parse</span>(<span class="params">self, response</span>):</span></span><br><span class="line">      quotes = response.css(<span class="string">&#x27;.quote&#x27;</span>)</span><br><span class="line">      <span class="keyword">for</span> quote <span class="keyword">in</span> quotes:</span><br><span class="line">          item = QuoteItem()</span><br><span class="line">          item[<span class="string">&#x27;text&#x27;</span>] = quote.css(<span class="string">&#x27;.text::text&#x27;</span>).extract_first()</span><br><span class="line">          item[<span class="string">&#x27;autnor&#x27;</span>] = quote.css(<span class="string">&#x27;.author::text&#x27;</span>).extract_first()</span><br><span class="line">          item[<span class="string">&#x27;tags&#x27;</span>] = quote.css(<span class="string">&#x27;.tags .tag::text&#x27;</span>).extract()</span><br><span class="line">          <span class="keyword">yield</span> item</span><br></pre></td></tr></table></figure>

<h5 id="后续-Request"><a href="#后续-Request" class="headerlink" title="后续 Request"></a>后续 Request</h5><p>上面操作实现了从初始页面抓取内容，下一页的内容需要从当前页面中找到信息来生成下一个请求</p>
<p>拉取页面到底部有个 Next 按钮，查看源码，它的链接是 /page/2/ 全链接是 <a target="_blank" rel="noopener" href="http://quotes.toscrape.com/page/2">http://quotes.toscrape.com/page/2</a> ，通过这个链接可以构造下一个请求</p>
<p>构造请求时需要用到 scrapy.Request，传递两个参数 url 和 callback</p>
<p>url：请求链接</p>
<p>callback：回调函数，会将请求的响应作为参数传递给这个回调函数，回调函数进行解析或生成下一个请求</p>
<p>修改后完整 Spider 类</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">from</span> tutorial.items <span class="keyword">import</span> QuoteItem</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">QuotesSpider</span>(<span class="params">scrapy.Spider</span>):</span></span><br><span class="line">  name = <span class="string">&#x27;quotes&#x27;</span></span><br><span class="line">  allowed_domains = [<span class="string">&#x27;quotes.toscrape.com&#x27;</span>]</span><br><span class="line">  start_urls = [<span class="string">&#x27;http://quotes.toscrape.com/&#x27;</span>]</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">parse</span>(<span class="params">self, response</span>):</span></span><br><span class="line">      quotes = response.css(<span class="string">&#x27;.quote&#x27;</span>)</span><br><span class="line">      <span class="keyword">for</span> quote <span class="keyword">in</span> quotes:</span><br><span class="line">          item = QuoteItem()</span><br><span class="line">          item[<span class="string">&#x27;text&#x27;</span>] = quote.css(<span class="string">&#x27;.text::text&#x27;</span>).extract_first()</span><br><span class="line">          item[<span class="string">&#x27;autnor&#x27;</span>] = quote.css(<span class="string">&#x27;.author::text&#x27;</span>).extract_first()</span><br><span class="line">          item[<span class="string">&#x27;tags&#x27;</span>] = quote.css(<span class="string">&#x27;.tags .tag::text&#x27;</span>).extract()</span><br><span class="line">          <span class="keyword">yield</span> item</span><br><span class="line">      <span class="comment">#利用选择器生成下一页的请求</span></span><br><span class="line">      <span class="built_in">next</span> = response.css(<span class="string">&#x27;.pager .next a::attr(&quot;href&quot;)&#x27;</span>).extract_first()</span><br><span class="line">      <span class="comment">#urljoin()方法可以将相对URL构造成一个绝对的URL</span></span><br><span class="line">      url = response.urljoin(<span class="built_in">next</span>)</span><br><span class="line">      <span class="comment">#返回下一页请求</span></span><br><span class="line">      <span class="keyword">yield</span>  scrapy.Request(url=url, callback=self.parse)</span><br></pre></td></tr></table></figure>

<h5 id="运行"><a href="#运行" class="headerlink" title="运行"></a>运行</h5><p>进入目录运行</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy crawl quotes</span><br></pre></td></tr></table></figure>

<p>运行后 Scrapy 输出当前的版本号及正在启动项目名称、settings.py  中一些重写后的配置、Middlewares、Pipelines</p>
<p>接下来就输出各个页面的抓取结果了</p>
<p>最后输出整个抓取过程的统计信息，请求字节数、请求次数、响应次数、完成原因等</p>
<h5 id="保存到文件"><a href="#保存到文件" class="headerlink" title="保存到文件"></a>保存到文件</h5><p>Scrapy 提供的 Feed Exports 可以将结果输出，如想将上面结果保存成 JSON 文件</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy crawl quotes -o quotes.json</span><br></pre></td></tr></table></figure>

<p>还可以每一个 Item 输出一行 JSON，输出后缀为 jl，为 jsonline 的缩写</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">scrapy crawl quotes -o quotes.jl</span><br><span class="line">或者 </span><br><span class="line">scrapy crawl quotes -o quotes.jsonline</span><br></pre></td></tr></table></figure>

<p>输出格式还支持很多种，如 csv、xml、pickle、marshal 等，还支持 ftp、s3等远程输出，还可以通过自定义 ItemExporter 来实现其他的输出</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scrapy crawl quotes -o quotes.csv</span><br><span class="line">scrapy crawl quotes -o quotes.xml</span><br><span class="line">scrapy crawl quotes -o quotes.pickle</span><br><span class="line">scrapy crawl quotes -o quotes.marshal</span><br><span class="line">scrapy crawl quotes -o ftp:&#x2F;&#x2F;user:pass@ftp.example.com&#x2F;path&#x2F;to&#x2F;quotes.csv</span><br></pre></td></tr></table></figure>

<p>通过 Scrapy 提供的 FeedExports，可以轻松的输出抓取结果到文件，想要更复杂的输出，如输出到数据库，可以使用 ItemPileline 来完成</p>
<h5 id="使用-Item-Pipeline"><a href="#使用-Item-Pipeline" class="headerlink" title="使用 Item Pipeline"></a>使用 Item Pipeline</h5><p>Item Pipeline 为项目管道，Item 生成后，它会自动被送到 Item Pipeline 进行处理，常用 Item Pipeline 来做如下操作</p>
<p>清理 HTML 数据、验证爬取数据，检查爬取字段、查重并丢弃重复内容、将爬取结果保存到数据库</p>
<p>要实现 Item Pipeline，只需要定义一个类并实现 process_item() 方法，启用 Item Pipeline 后，Item Pipeline 会自动调用这个方法，process_item() 方法必须返回包含数据的字典或 Item 对象或者抛出 DropItem 异常</p>
<p>process_item() 有两个参数，一个参数是 item，每次 Spider 生成的 Item 都会作为参数传递过来，第二个参数是 Spider，就是 Spider 的实例</p>
<p>实现 Item Pipeline，筛掉 text 长度大于 50 的Item，并将结果保存到 MongoDB</p>
<p>修改项目里的 pipelines.py 文件</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scrapy.exceptions <span class="keyword">import</span> DropItem</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TextPipeline</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">      self.limit = <span class="number">50</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">process_item</span>(<span class="params">self, item, spider</span>):</span></span><br><span class="line">      <span class="keyword">if</span> item[<span class="string">&#x27;text&#x27;</span>]:</span><br><span class="line">          <span class="keyword">if</span> <span class="built_in">len</span>(item[<span class="string">&#x27;text&#x27;</span>] &gt; self.limit):</span><br><span class="line">              <span class="comment">#截断后拼省略号返回</span></span><br><span class="line">              item[<span class="string">&#x27;text&#x27;</span>] = item[<span class="string">&#x27;text&#x27;</span>][<span class="number">0</span>:self.limit].rstrip() + <span class="string">&#x27;...&#x27;</span></span><br><span class="line">      <span class="keyword">else</span>:</span><br><span class="line">          <span class="keyword">return</span> DropItem(<span class="string">&#x27;Missing Text&#x27;</span>)</span><br></pre></td></tr></table></figure>

<p>接下来将处理后的 item 存入 MongoDB，定义另外一个 Pipeline，MongoPipeline</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pymongo</span><br><span class="line"><span class="keyword">from</span> scrapy.exceptions <span class="keyword">import</span> DropItem</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MongoPipeline</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, mongo_uri, mongo_db</span>):</span></span><br><span class="line">      self.mongo_uri = mongo_uri</span><br><span class="line">      self.mongo_db = mongo_db</span><br><span class="line"><span class="meta">  @classmethod</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">from_crawler</span>(<span class="params">cls, crawler</span>):</span></span><br><span class="line">      <span class="keyword">return</span> cls(</span><br><span class="line">          mongo_uri=crawler.settings.get(<span class="string">&#x27;MONGO_URI&#x27;</span>),</span><br><span class="line">          mongo_db=crawler.settings.get(<span class="string">&#x27;MONGODB_DATABASE&#x27;</span>)</span><br><span class="line">      )</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">open_spider</span>(<span class="params">self, spider</span>):</span></span><br><span class="line">      self.cline = pymongo.MongoClient(self.mongo_uri)</span><br><span class="line">      self.db = self.cline[self.mongo_db]</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">process_item</span>(<span class="params">self, item, spider</span>):</span></span><br><span class="line">      name = item.__class__.__name__</span><br><span class="line">      self.db[name].insert(<span class="built_in">dict</span>(item))</span><br><span class="line">      <span class="keyword">return</span> item</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">close_spider</span>(<span class="params">self, spider</span>):</span></span><br><span class="line">      self.cline.close()</span><br></pre></td></tr></table></figure>

<p>MongoPipeline 实现了另外几个方法</p>
<p>from_crawler：是一个类方法，用 @classmethod 标识，是一种依赖注入的方式，参数是 crawler，通过 crawler 可以拿到全局配置的每个配置信息，全局配置 settings.py 定义了 MongoDB 连接需要的地址和数据库名称，拿到配置信息返回类对象即可，这方法主要用来获取 setting.py 中的配置</p>
<p>open_spider：当 Spider 开启时，这个方法被调用，上面程序中主要进行了一些初始化操作</p>
<p>close_spider：当 Spider 关闭时，这个方法被调用，上面程序中将数据库连接关闭</p>
<p>process_item() 方法则执行了数据库插入操作</p>
<p>定义好了 TextPipeline 和 MongoPipeline 两个类后，需要在 settings.py 中使用他们，MongoDB 的连接信息还需要定义</p>
<p>settings.py 中加入</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">TIME_PIPELINES = &#123;</span><br><span class="line">	<span class="string">&#x27;tutorial.pipelines.TextPipeline&#x27;</span>: <span class="number">300</span>,</span><br><span class="line">	<span class="string">&#x27;tutorial.pipelines.MongoPipeline&#x27;</span>: <span class="number">400</span></span><br><span class="line">&#125;</span><br><span class="line">MONGO_URI = <span class="string">&#x27;localhost&#x27;</span></span><br><span class="line">MONGO_DB = <span class="string">&#x27;tutorial&#x27;</span></span><br></pre></td></tr></table></figure>

<p>赋值 TIME_PIPELINES 字典，键名是 Pipeline 的类名称，键值是调用优先级，是一个数字，数字越小则对应 Pipeline 越先被调用</p>
<p>重新执行爬取</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy crawl quotes</span><br></pre></td></tr></table></figure>

<h4 id="13-3-Selecotr-用法"><a href="#13-3-Selecotr-用法" class="headerlink" title="13.3 Selecotr 用法"></a>13.3 Selecotr 用法</h4><p>Scrapy 还提供了自己的数据提取方法，即 Selector，Selector 是基于 lxml 来构建的，支持 XPath 选择器、CSS 选择器以及正则表达式</p>
<h5 id="直接使用"><a href="#直接使用" class="headerlink" title="直接使用"></a>直接使用</h5><p>Selector 是一个可以独立使用的模块，可以利用 Selector 来构建一个选择器对象，然后调用它的相关方法 如</p>
<p>xpath()、css() 等来提取数据</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scrapy <span class="keyword">import</span> Selector</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">  body = <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">  &lt;html&gt;</span></span><br><span class="line"><span class="string">      &lt;head&gt;</span></span><br><span class="line"><span class="string">          &lt;title&gt;Hello World&lt;/title&gt;</span></span><br><span class="line"><span class="string">      &lt;/head&gt;</span></span><br><span class="line"><span class="string">  &lt;/html&gt;</span></span><br><span class="line"><span class="string">  &#x27;&#x27;&#x27;</span></span><br><span class="line">  selector = Selector(text=body)</span><br><span class="line">  title = selector.xpath(<span class="string">&#x27;//title/text()&#x27;</span>).extract_first()</span><br><span class="line">  title1 = selector.xpath(<span class="string">&#x27;//title/text()&#x27;</span>).extract()</span><br><span class="line">  title2 = selector.xpath(<span class="string">&#x27;//title/text()&#x27;</span>)</span><br><span class="line">  print(title)</span><br><span class="line">  print(title1)</span><br><span class="line">  print(title2)</span><br><span class="line"><span class="comment">#Hello World</span></span><br><span class="line"><span class="comment">#[&#x27;Hello World&#x27;]</span></span><br><span class="line"><span class="comment">#[&lt;Selector xpath=&#x27;//title/text()&#x27; data=&#x27;Hello World&#x27;&gt;]</span></span><br></pre></td></tr></table></figure>

<h5 id="Scrapy-shell"><a href="#Scrapy-shell" class="headerlink" title="Scrapy shell"></a>Scrapy shell</h5><p>借助 Scrapy shell 来模拟 Scrapy 请求的过程 ，讲解相关的提取方法</p>
<p>官方文档的样例来做演示 <a target="_blank" rel="noopener" href="https://docs.scrapy.org/en/latest/_static/selectors-sample1.html">https://docs.scrapy.org/en/latest/_static/selectors-sample1.html</a></p>
<p>开启 Scrapy shell</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy shell https:&#x2F;&#x2F;docs.scrapy.org&#x2F;en&#x2F;latest&#x2F;_static&#x2F;selectors-sample1.html</span><br></pre></td></tr></table></figure>

<p>就进入 Scrapy shell 模式，这过程其实是 Scrapy 发起了一次请求，请求的URL就是命令行下输入的 URL，然后把一些可操作的变量传递给我们，如request、response 等</p>
<img src="Python3网络爬虫开发实战-Scrapy框架使用/WeChatae25474d863d90da3fe12abbc4a54da0.png" alt="WeChatae25474d863d90da3fe12abbc4a54da0" style="zoom:80%;" />

<p>可以在命令模式下输入命令调用对象的一些操作方法，回车之后实时显示结果</p>
<p>查看页面源码</p>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">html</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">head</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">base</span> <span class="attr">href</span>=<span class="string">&#x27;http://example.com/&#x27;</span> /&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">title</span>&gt;</span>Example website<span class="tag">&lt;/<span class="name">title</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;/<span class="name">head</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">body</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">div</span> <span class="attr">id</span>=<span class="string">&#x27;images&#x27;</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">&#x27;image1.html&#x27;</span>&gt;</span>Name: My image 1 <span class="tag">&lt;<span class="name">br</span> /&gt;</span><span class="tag">&lt;<span class="name">img</span> <span class="attr">src</span>=<span class="string">&#x27;image1_thumb.jpg&#x27;</span> /&gt;</span><span class="tag">&lt;/<span class="name">a</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">&#x27;image2.html&#x27;</span>&gt;</span>Name: My image 2 <span class="tag">&lt;<span class="name">br</span> /&gt;</span><span class="tag">&lt;<span class="name">img</span> <span class="attr">src</span>=<span class="string">&#x27;image2_thumb.jpg&#x27;</span> /&gt;</span><span class="tag">&lt;/<span class="name">a</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">&#x27;image3.html&#x27;</span>&gt;</span>Name: My image 3 <span class="tag">&lt;<span class="name">br</span> /&gt;</span><span class="tag">&lt;<span class="name">img</span> <span class="attr">src</span>=<span class="string">&#x27;image3_thumb.jpg&#x27;</span> /&gt;</span><span class="tag">&lt;/<span class="name">a</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">&#x27;image4.html&#x27;</span>&gt;</span>Name: My image 4 <span class="tag">&lt;<span class="name">br</span> /&gt;</span><span class="tag">&lt;<span class="name">img</span> <span class="attr">src</span>=<span class="string">&#x27;image4_thumb.jpg&#x27;</span> /&gt;</span><span class="tag">&lt;/<span class="name">a</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">&#x27;image5.html&#x27;</span>&gt;</span>Name: My image 5 <span class="tag">&lt;<span class="name">br</span> /&gt;</span><span class="tag">&lt;<span class="name">img</span> <span class="attr">src</span>=<span class="string">&#x27;image5_thumb.jpg&#x27;</span> /&gt;</span><span class="tag">&lt;/<span class="name">a</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;/<span class="name">body</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">html</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h5 id="XPath-选择器"><a href="#XPath-选择器" class="headerlink" title="XPath 选择器"></a>XPath 选择器</h5><p>进入 Scrapy shell 之后，操作 response 这个变量来进行分析</p>
<p>response 有一个属性 selector，调用 response.selector 返回的内容就相当于用 response 的body 构造了一个 Selector 对象，通过这个 Selector 对象可以调用解析方法如 xpath()、css() 等</p>
<ul>
<li>查询嵌套查询</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">#从html提取a节点</span><br><span class="line">&gt;&gt;&gt; result &#x3D; response.selector.xpath(&#39;&#x2F;&#x2F;a&#39;)</span><br><span class="line">&gt;&gt;&gt; result</span><br><span class="line">[&lt;Selector xpath&#x3D;&#39;&#x2F;&#x2F;a&#39; data&#x3D;&#39;&lt;a href&#x3D;&quot;image1.html&quot;&gt;Name: My image ...&#39;&gt;, </span><br><span class="line">&lt;Selector xpath&#x3D;&#39;&#x2F;&#x2F;a&#39; data&#x3D;&#39;&lt;a href&#x3D;&quot;image2.html&quot;&gt;Name: My image ...&#39;&gt;, </span><br><span class="line">&lt;Selector xpath&#x3D;&#39;&#x2F;&#x2F;a&#39; data&#x3D;&#39;&lt;a href&#x3D;&quot;image3.html&quot;&gt;Name: My image ...&#39;&gt;, </span><br><span class="line">&lt;Selector xpath&#x3D;&#39;&#x2F;&#x2F;a&#39; data&#x3D;&#39;&lt;a href&#x3D;&quot;image4.html&quot;&gt;Name: My image ...&#39;&gt;, </span><br><span class="line">&lt;Selector xpath&#x3D;&#39;&#x2F;&#x2F;a&#39; data&#x3D;&#39;&lt;a href&#x3D;&quot;image5.html&quot;&gt;Name: My image ...&#39;&gt;]</span><br><span class="line">#从a节点提取img节点</span><br><span class="line">&gt;&gt;&gt; result.xpath(&#39;.&#x2F;img&#39;)</span><br></pre></td></tr></table></figure>

<p>选择器前面加 . (点)，代表提取元素内部的数据，没有加点则代表从根节点开始提取</p>
<ul>
<li>提取内容</li>
</ul>
<p>Scrapy 提供了两个使用的快捷方法 </p>
<p>response.xpath() 和 response.css() 功能等同于 response.selector.xpath() 和 response.selector.css()</p>
<p>现在得到的是 SelectorList 类型的变量，该变量是由 Selector 对象组成的列表，可以用索引单独提取其中某个 Selector 元素</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; result[0]</span><br><span class="line">&lt;Selector xpath&#x3D;&#39;&#x2F;&#x2F;a&#39; data&#x3D;&#39;&lt;a href&#x3D;&quot;image1.html&quot;&gt;Name: My image ...&#39;&gt;</span><br></pre></td></tr></table></figure>

<p>提取内容 extract() 方法</p>
<p>加一层 /text() 就可以获取节点的内部文本，或者加一层 /@href 就可获取节点的 href 属性，@符号后面就是要获取的属性名称</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; result.extract()</span><br><span class="line">&gt;&gt;&gt; response.xpath(&#39;&#x2F;&#x2F;a&#x2F;text()&#39;).extract()</span><br></pre></td></tr></table></figure>

<ul>
<li>提取单个元素 extract_first() 不用担心数组越界问题</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">#将第一个匹配结果取出来 extract_first()</span><br><span class="line">&gt;&gt;&gt; result.xpath(&#39;&#x2F;&#x2F;a[@href&#x3D;&quot;image1.html&quot;]&#x2F;text()&#39;).extract_first()</span><br></pre></td></tr></table></figure>

<ul>
<li>提取不到使用默认值</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; result.xpath(&#39;&#x2F;&#x2F;a[@href&#x3D;&quot;image1.html&quot;]&#x2F;text()&#39;).extract_first(&#39;Default Image&#39;)</span><br><span class="line">#默认值 Default Image</span><br></pre></td></tr></table></figure>

<h5 id="CSS-选择器"><a href="#CSS-选择器" class="headerlink" title="CSS 选择器"></a>CSS 选择器</h5><p>选取 a 节点</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; response.css(&#39;a&#39;)</span><br><span class="line">[&lt;Selector xpath&#x3D;&#39;descendant-or-self::a&#39; data&#x3D;&#39;&lt;a href&#x3D;&quot;image1.html&quot;&gt;Name: My image ...&#39;&gt;, &lt;Selector xpath&#x3D;&#39;descendant-or-self::a&#39; data&#x3D;&#39;&lt;a href&#x3D;&quot;image2.html&quot;&gt;Name: My image ...&#39;&gt;, &lt;Selector xpath&#x3D;&#39;descendant-or-self::a&#39; data&#x3D;&#39;&lt;a href&#x3D;&quot;image3.html&quot;&gt;Name: My image ...&#39;&gt;, &lt;Selector xpath&#x3D;&#39;descendant-or-self::a&#39; data&#x3D;&#39;&lt;a href&#x3D;&quot;image4.html&quot;&gt;Name: My image ...&#39;&gt;, &lt;Selector xpath&#x3D;&#39;descendant-or-self::a&#39; data&#x3D;&#39;&lt;a href&#x3D;&quot;image5.html&quot;&gt;Name: My image ...&#39;&gt;]</span><br><span class="line">&gt;&gt;&gt; response.css(&#39;a&#39;).extract()</span><br><span class="line">&gt;&gt;&gt; response.css(&#39;a&#39;).extract_first()</span><br></pre></td></tr></table></figure>

<ul>
<li>属性选择和嵌套选择</li>
</ul>
<p>查找 a 节点内部 img 节点，只需要加一个空格和img 即可</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; response.css(&#39;a[href&#x3D;&quot;image1.html&quot;]&#39;).extract()</span><br><span class="line">[&#39;&lt;a href&#x3D;&quot;image1.html&quot;&gt;Name: My image 1 &lt;br&gt;&lt;img src&#x3D;&quot;image1_thumb.jpg&quot;&gt;&lt;&#x2F;a&gt;&#39;]</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; response.css(&#39;a[href&#x3D;&quot;image1.html&quot;] img&#39;).extract()</span><br><span class="line">[&#39;&lt;img src&#x3D;&quot;image1_thumb.jpg&quot;&gt;&#39;]</span><br></pre></td></tr></table></figure>

<ul>
<li>节点内部文本和属性获取</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; response.css(&#39;a[href&#x3D;&quot;image1.html&quot;]::text&#39;).extract_first()</span><br><span class="line">&#39;Name: My image 1 &#39;</span><br><span class="line">&gt;&gt;&gt; response.css(&#39;a[href&#x3D;&quot;image1.html&quot;] img::attr(src)&#39;).extract_first()</span><br><span class="line">&#39;image1_thumb.jpg&#39;</span><br></pre></td></tr></table></figure>

<p>获取文本和属性要用 ::text 和 ::attr() 的写法</p>
<ul>
<li>CSS 选择器 XPath 选择器嵌套使用</li>
</ul>
<p>获取所有 img 节点的 src 属性</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; response.xpath(&#39;&#x2F;&#x2F;a&#39;).css(&#39;img&#39;).xpath(&#39;@src&#39;).extract()</span><br><span class="line">[&#39;image1_thumb.jpg&#39;, &#39;image2_thumb.jpg&#39;, &#39;image3_thumb.jpg&#39;, &#39;image4_thumb.jpg&#39;, &#39;image5_thumb.jpg&#39;]</span><br></pre></td></tr></table></figure>

<h5 id="正则匹配"><a href="#正则匹配" class="headerlink" title="正则匹配"></a>正则匹配</h5><p>Scrapy 还支持正则匹配</p>
<p>a 节点中文本类似于 Name: My image 1，现在提取 Name: 后面内容</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; response.xpath(&#39;&#x2F;&#x2F;a&#x2F;text()&#39;).re(&#39;Name:\s(.*)&#39;)</span><br><span class="line">[&#39;My image 1 &#39;, &#39;My image 2 &#39;, &#39;My image 3 &#39;, &#39;My image 4 &#39;, &#39;My image 5 &#39;]</span><br></pre></td></tr></table></figure>

<p>给 re() 方法传一个正则表达式，(.*) 就是要匹配的内容</p>
<p>同时存在两个分组，结果会按序输出</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; response.xpath(&#39;&#x2F;&#x2F;a&#x2F;text()&#39;).re(&#39;(.*?)\s(.*)&#39;)</span><br><span class="line">[&#39;Name:&#39;, &#39;My image 1 &#39;, &#39;Name:&#39;, &#39;My image 2 &#39;, &#39;Name:&#39;, &#39;My image 3 &#39;, &#39;Name:&#39;, &#39;My image 4 &#39;, &#39;Name:&#39;, &#39;My image 5 &#39;]</span><br></pre></td></tr></table></figure>

<p>类似 extract_first() 方法 re_first() 方法可以选择列表第一个元素</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; response.xpath(&#39;&#x2F;&#x2F;a&#x2F;text()&#39;).re_first(&#39;Name:\s(.*)&#39;)</span><br><span class="line">&#39;My image 1 &#39;</span><br></pre></td></tr></table></figure>

<p>response 对象不能直接调用 re() 和 re_first() ，如果想要对全文进行正则匹配，可以先调用 xpath() 方法再正则匹配</p>
<h4 id="13-4-Spider-用法"><a href="#13-4-Spider-用法" class="headerlink" title="13.4 Spider 用法"></a>13.4 Spider 用法</h4><h5 id="Spider-类"><a href="#Spider-类" class="headerlink" title="Spider 类"></a>Spider 类</h5><p>定义的 Spider 是继承自 scrapy.spiders.Spider</p>
<p>scrapy.spiders.Spider 类提供了 start_requests() 方法的默认实现，读取并请求 start_urls 属性，并根据返回的结果调用 parse() 方法解析结果。</p>
<p>name：爬虫名称，一般以网站域名名称来命名，Spider 爬取 mywebsite.com，命名为 mywebsite</p>
<p>allow_domains：允许爬取的域名，是可选配置，不在此范围的链接不会被跟进爬取</p>
<p>start_urls：起始 URL 列表，没有实现 start_requests() 方法时，默认会从这个列表开始抓取</p>
<p>custom_settings：一个字典，专属本 Spider 的配置，会覆盖全局的设置，此设置必须在初始化前被更新，必须定义成类变量</p>
<p>crawler：是由 from_crawler() 方法设置的，代表本 Spider 类对应的 Crawler 对象，Crawler 对象包含了很多项目组件，利用它可以获取项目一些配置信息，最常见的获取项目的设置信息，即 settings</p>
<p>settings：一个Settings 对象，利用它可以直接获取项目全局设置变量</p>
<ul>
<li>start_requests()</li>
</ul>
<p>此方法用于生成初始请求，必须返回一个可迭代对象。此方法会默认使用 start_urls 里面的URL 来构造 Request，而且 Request 是 GET请求方式，如果想在启动时以 POST 方式访问某个站点，可以直接重写这个方法，发送 POST 请求时使用 FormRequest</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ImagesSpider</span>(<span class="params">Spider</span>):</span></span><br><span class="line">  name = <span class="string">&#x27;images&#x27;</span></span><br><span class="line">  allowed_domains = [<span class="string">&#x27;images.so.com&#x27;</span>]</span><br><span class="line">  start_urls = [<span class="string">&#x27;http://images.so.com/&#x27;</span>]</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">start_requests</span>(<span class="params">self</span>):</span></span><br><span class="line">      data = &#123;<span class="string">&#x27;ch&#x27;</span>: <span class="string">&#x27;photography&#x27;</span>, <span class="string">&#x27;listtype&#x27;</span>: <span class="string">&#x27;new&#x27;</span>&#125;</span><br><span class="line">      base_url = <span class="string">&#x27;https://image.so.com/zjl?&#x27;</span></span><br><span class="line">      <span class="keyword">for</span> page <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, self.settings.get(<span class="string">&#x27;MAX_PAGE&#x27;</span>) + <span class="number">1</span>):</span><br><span class="line">          data[<span class="string">&#x27;sn&#x27;</span>] = page * <span class="number">30</span></span><br><span class="line">          <span class="comment">#利用urlencode()方法将字典转化为URL的GET参数</span></span><br><span class="line">          params = urlencode(data)</span><br><span class="line">          url = base_url + params</span><br><span class="line">          <span class="keyword">yield</span> Request(url, self.parse)</span><br></pre></td></tr></table></figure>

<ul>
<li>parse()</li>
</ul>
<p>当 Response 没指定回调函数时，该方法被调用，负责处理 Response，处理返回结果，并从中提取想要的数据和下一步请求。需要返回一个包含 Request 或 Item 的可迭代对象</p>
<p>close()：当 Spider 关闭时，该方法被调用，这里一般会定义释放资源的一些操作</p>
<h4 id="13-5-Downloader-Middleware"><a href="#13-5-Downloader-Middleware" class="headerlink" title="13.5 Downloader Middleware"></a>13.5 Downloader Middleware</h4><p>下载中间件，处于 Scrapy 的 Request 和 Response 之间的处理模块</p>
<p>Scheduler 从队列中拿出一个 Request 发送给 Downloader 执行下载，这过程会经过 Downloader Middlewares 的处理</p>
<p>当 Downloader 将 Request 下载完成得到的 Response 返回给 Spider 时会再次经过  Downloader Middlewares 处理</p>
<p>修改 User-Agent、处理重定向、设置代理、失败重试、设置Cookies等功能都需要借助  Downloader Middlewares 实现</p>
<p>Scrapy 已经提供了许多  Downloader Middlewares，比如失败重试、自动重定向等功能的 Middleware，被 DOWNLOADER_MIDDLEWARES_BASE 变量定义，是一个字典格式</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">	&#39;scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware&#39;: 100</span><br><span class="line">	...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>键值是 优先级，数字越小会被优先调用</p>
<p>Scrapy 提供了一个设置变量 DOWNLOADER_MIDDLEWARES，直接修改这个变量就可以添加自己定义的  Downloader Middlewares</p>
<h5 id="核心方法"><a href="#核心方法" class="headerlink" title="核心方法"></a>核心方法</h5><p>每个  Downloader Middlewares 都定义了一个或多个方法的类，核心方法三个</p>
<p>process_request(request, spider)</p>
<p>process_response(request, response, spider)</p>
<p>process_exception(request, exception, spider)</p>
<p>只需要实现一个方法，就可以定义一个 Downloader Middlewares </p>
<ul>
<li>process_request(request, spider)</li>
</ul>
<p>Request 被 Scrapy 引擎调度给Downloader之前，会被调用</p>
<p>可以用 process_request 对 Request 方法进行处理。返回值必须为 None、Response对象、Request 对象之一，或者抛出 IgnoreRequest 异常</p>
<ul>
<li>process_response(request, response, spider)</li>
</ul>
<p>Downloader 执行 Request 下载之后，会得到对应的 Response，Scrapy 引擎便会将 Response 发送给 Spider 进行解析。发送之前，可以用 process_response 方法对 Response 进行处理。返回值必须为 None、Response对象、Request 对象之一，或者抛出 IgnoreRequest 异常</p>
<ul>
<li>process_exception(request, exception, spider)</li>
</ul>
<p>Downloader 或 process_rqeuest() 方法抛出异常时被调用</p>
<h5 id="实战"><a href="#实战" class="headerlink" title="实战"></a>实战</h5><p>新建项目</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy startproject scrapydownloadertest</span><br></pre></td></tr></table></figure>

<p>进入项目新建 Spider， 名为 httpbin</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd scrapydownloadertest</span><br><span class="line">scrapy genspider httpbin httpbin.org</span><br></pre></td></tr></table></figure>

<p>修改内容</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">HttpbinSpider</span>(<span class="params">scrapy.Spider</span>):</span></span><br><span class="line">  name = <span class="string">&#x27;httpbin&#x27;</span></span><br><span class="line">  allowed_domains = [<span class="string">&#x27;httpbin.org&#x27;</span>]</span><br><span class="line">  start_urls = [<span class="string">&#x27;http://httpbin.org/get&#x27;</span>]</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">parse</span>(<span class="params">self, response</span>):</span></span><br><span class="line">      self.logger.debug(response.text)</span><br></pre></td></tr></table></figure>

<p>运行  <code>scrapy crawl httpbin</code></p>
<p>运行结果包含 Scrapy 发送的 Request 信息 </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"> DEBUG: &#123;</span><br><span class="line">  &quot;args&quot;: &#123;&#125;,</span><br><span class="line">  &quot;headers&quot;: &#123;</span><br><span class="line">    &quot;Accept&quot;: &quot;text&#x2F;html,application&#x2F;xhtml+xml,application&#x2F;xml;q&#x3D;0.9,*&#x2F;*;q&#x3D;0.8&quot;,</span><br><span class="line">    &quot;Accept-Encoding&quot;: &quot;gzip, deflate&quot;,</span><br><span class="line">    &quot;Accept-Language&quot;: &quot;en&quot;,</span><br><span class="line">    &quot;Host&quot;: &quot;httpbin.org&quot;,</span><br><span class="line">    &quot;User-Agent&quot;: &quot;Scrapy&#x2F;2.4.1 (+https:&#x2F;&#x2F;scrapy.org)&quot;,</span><br><span class="line">    &quot;X-Amzn-Trace-Id&quot;: &quot;Root&#x3D;1-625fa44b-7de6f90f1696c8d44ba334ab&quot;</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;origin&quot;: &quot;14.155.91.98&quot;,</span><br><span class="line">  &quot;url&quot;: &quot;http:&#x2F;&#x2F;httpbin.org&#x2F;get&quot;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li>修改 User-Agent 两种方式</li>
</ul>
<p>第一种修改 settings 里的 USER_AGENT 变量</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">USERAGENT &#x3D; &#39;xxxx&#39;</span><br></pre></td></tr></table></figure>

<p>如果想设置更灵活，比如随机 User-Agent，需要借助 Downloader Middleware</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RandomUserAgentMiddleware</span>():</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">      self.user_agents = [</span><br><span class="line">          <span class="string">&#x27;xxx1&#x27;</span>,</span><br><span class="line">          <span class="string">&#x27;xxx2&#x27;</span>,</span><br><span class="line">      ]</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">process_request</span>(<span class="params">self, request, spider</span>):</span></span><br><span class="line">      request.headers[<span class="string">&#x27;User-Agent&#x27;</span>] = random.choice(self.user_agents)</span><br></pre></td></tr></table></figure>

<p>要使这个 Downloader Middleware 生效还要去调用它，在 settings.py 中，将 DOWNLOADER_MIDDLEWARES 取消注释</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">DOWNLOADER_MIDDLEWARES = &#123;</span><br><span class="line"><span class="string">&#x27;scrapydownloadertest.middlewares.RandomUserAgentMiddleware&#x27;</span>: <span class="number">543</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>再次运行发现 User-Agent 被修改了 </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">DEBUG: &#123;</span><br><span class="line">  &quot;args&quot;: &#123;&#125;,</span><br><span class="line">  &quot;headers&quot;: &#123;</span><br><span class="line">    &quot;Accept&quot;: &quot;text&#x2F;html,application&#x2F;xhtml+xml,application&#x2F;xml;q&#x3D;0.9,*&#x2F;*;q&#x3D;0.8&quot;,</span><br><span class="line">    &quot;Accept-Encoding&quot;: &quot;gzip, deflate&quot;,</span><br><span class="line">    &quot;Accept-Language&quot;: &quot;en&quot;,</span><br><span class="line">    &quot;Host&quot;: &quot;httpbin.org&quot;,</span><br><span class="line">    &quot;User-Agent&quot;: &quot;xxx2&quot;,</span><br><span class="line">    &quot;X-Amzn-Trace-Id&quot;: &quot;Root&#x3D;1-625fa838-4f44d9361feb25107824c407&quot;</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;origin&quot;: &quot;14.155.91.98&quot;,</span><br><span class="line">  &quot;url&quot;: &quot;http:&#x2F;&#x2F;httpbin.org&#x2F;get&quot;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li>修改响应</li>
</ul>
<p>RandomUserAgentMiddleware 中添加 </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">process_response</span>(<span class="params">self, response, spider</span>):</span></span><br><span class="line">    response.status = <span class="number">201</span></span><br><span class="line">    <span class="keyword">return</span> response</span><br></pre></td></tr></table></figure>

<p>再 Spider 的 parse() 方法添加输出</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">self.logger.debug(&#39;Status code:&#39; + str(response.status))</span><br></pre></td></tr></table></figure>

<h4 id="13-6-Spider-Middleware"><a href="#13-6-Spider-Middleware" class="headerlink" title="13.6 Spider Middleware"></a>13.6 Spider Middleware</h4><p>Spider Middleware 三个作用</p>
<p>Response 发送给 Spider 之前对 Response 进行处理</p>
<p>Request 发送给 Scheduler 之前对 Request 进行处理</p>
<p>Item 发送给 Item Pipeline 之前对 Item 进行处理</p>
<p>Scrapy 已经提供了许多 Spider Middleware，被 SPIDER_MIDDLEWARES_BASE 变量定义</p>
<p>Spider Middleware 4个核心方法，只需要实现其中一个方法就可以定义一个 Spider Middleware</p>
<ul>
<li>process_spider_input(response, spider)</li>
</ul>
<p>Response 被 Spider Middleware 处理时被调用</p>
<p>返回 None 或者抛出异常</p>
<ul>
<li>process_spider_output(response, result, spider)</li>
</ul>
<p>当 Spider 处理 Response 返回结果时被调用</p>
<p>必须返回包含 Request 或 Item 对象的可迭代对象</p>
<ul>
<li>process_spider_exception(response, exception, spider)</li>
</ul>
<p>Spider 或 process_spider_input 方法抛出异常时调用</p>
<ul>
<li>process_start_requests(start_requests, spider)</li>
</ul>
<h4 id="13-6-Item-Pipeline"><a href="#13-6-Item-Pipeline" class="headerlink" title="13.6 Item Pipeline"></a>13.6 Item Pipeline</h4><p>参考使用 Item Pipeline</p>
<h4 id="爬取-360摄影美图"><a href="#爬取-360摄影美图" class="headerlink" title="爬取 360摄影美图"></a>爬取 360摄影美图</h4><h5 id="构造请求"><a href="#构造请求" class="headerlink" title="构造请求"></a>构造请求</h5><h5 id="MongoDB存储"><a href="#MongoDB存储" class="headerlink" title="MongoDB存储"></a>MongoDB存储</h5><h5 id="MySQL-存储"><a href="#MySQL-存储" class="headerlink" title="MySQL 存储"></a>MySQL 存储</h5><h5 id="Image-Pipeline"><a href="#Image-Pipeline" class="headerlink" title="Image Pipeline"></a>Image Pipeline</h5><p>Scrapy 提供了专门处理下载的 Pipeline，包括下载文件和图片，下载过程支持异步和多线程</p>
<p>官方文档地址 <a target="_blank" rel="noopener" href="https://doc.scrapy.org/en/latest/topics/media-pipeline.html">https://doc.scrapy.org/en/latest/topics/media-pipeline.html</a></p>
<p>首先定义文件存储的路径，需要定义一个 IMAGES_STORE 变量，在settings.py 添加代码</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">IMAGES_STORE &#x3D; &#39;.&#x2F;images&#39;</span><br></pre></td></tr></table></figure>

<p>下载的图片都会保存到当前路径的 images 子文件夹下</p>
<p>内置的 ImagesPipeline 会默认读取 Item 的 image_urls 字段，并认为字段是一个列表形式，它会遍历 Item 的 image_urls 字段，然后取出每个 URL 进行图片下载</p>
<p>现在生成的 Item 的图片链接字段并不是 image_urls 字段表示的，也不是列表形式，而是单个 URL</p>
<p>所以为了实现下载，需要重新定义下载部分逻辑，即要自定义 ImagePipeline，继承自内置的 ImagesPipeline</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ImagePipeline</span>(<span class="params">ImagesPipeline</span>):</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">file_path</span>(<span class="params">self, request, response=<span class="literal">None</span>, info=<span class="literal">None</span></span>):</span></span><br><span class="line">      url = request.url</span><br><span class="line">      file_name = url.split(<span class="string">&#x27;/&#x27;</span>)[<span class="number">-1</span>]</span><br><span class="line">      <span class="keyword">return</span> file_name</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">item_completed</span>(<span class="params">self, results, item, info</span>):</span></span><br><span class="line">      image_paths = [x[<span class="string">&#x27;path&#x27;</span>] <span class="keyword">for</span> ok, x <span class="keyword">in</span> results <span class="keyword">if</span> ok]</span><br><span class="line">      <span class="keyword">if</span> <span class="keyword">not</span> image_paths:</span><br><span class="line">          <span class="keyword">raise</span> DropItem(<span class="string">&#x27;Image Downloaded Failed&#x27;</span>)</span><br><span class="line">      <span class="keyword">return</span> item</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">get_media_requests</span>(<span class="params">self, item, info</span>):</span></span><br><span class="line">      <span class="keyword">yield</span> Request(item[<span class="string">&#x27;url&#x27;</span>])</span><br></pre></td></tr></table></figure>

<ul>
<li>get_media_requests</li>
</ul>
<p>第一个参数 item 是爬取生成的 Item 对象，将它的 url 字段取出来，然后直接生成 Request对象，将此 Request 加入到调度队列，等待被调度，执行下载</p>
<ul>
<li>file_path</li>
</ul>
<p>这个方法用来返回保存的文件名，直接将图片链接最后一部分当作文件名即可，用 split 函数分割拼接并提取最后一部分</p>
<ul>
<li>item_completed</li>
</ul>
<p>它是单个 Item 完成下载时的处理方法，并不是每张图都会下载成功，需要分析下载结果剔除下载失败的图片，没下载成功就不需要保存 Item 到数据库</p>
<p>方法第一个参数 results 就是该 Item 对应下载结果，是一个列表形式，列表每一个元素是一个元组，其中包含了下载成功或失败的信息</p>
<p>这里遍历下载结果找出所有成功的下载列表。</p>
<h4 id="13-8-Scrapy-对接-Selenium"><a href="#13-8-Scrapy-对接-Selenium" class="headerlink" title="13.8 Scrapy 对接 Selenium"></a>13.8 Scrapy 对接 Selenium</h4><p>Scrapy 抓取页面的方式和 requests 库类似，都是直接模拟 HTTP 请求</p>
<p>前面抓取 JavaScript 渲染的页面有两种方式。一种是分析 Ajax 请求，找到对应的接口抓取，Scrapy 同样也可以用这种方法抓取。另一种是用 Selenium 或 Splash 模拟浏览器进行抓取，不需要关心页面后台发生的请求，也不需要分析渲染过程，只需要关心页面最终结果，即可见即可爬</p>
<h5 id="对接-Selenium"><a href="#对接-Selenium" class="headerlink" title="对接 Selenium"></a>对接 Selenium</h5><p>对接 Selenium 进行抓取，采用 Downloader Middleware 来实现，在 Middleware 里面的 process_request() 方法里对每个抓取请求进行处理，启动浏览器进行页面渲染，再将渲染后的结果构造一个 HtmlResponse 对象返回</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line"><span class="keyword">from</span> selenium.common.exceptions <span class="keyword">import</span> TimeoutException</span><br><span class="line"><span class="keyword">from</span> selenium.webdriver.common.by <span class="keyword">import</span> By</span><br><span class="line"><span class="keyword">from</span> selenium.webdriver.support.ui <span class="keyword">import</span> WebDriverWait</span><br><span class="line"><span class="keyword">from</span> selenium.webdriver.support <span class="keyword">import</span> expected_conditions <span class="keyword">as</span> EC</span><br><span class="line"><span class="keyword">from</span> scrapy.http <span class="keyword">import</span> HtmlResponse</span><br><span class="line"><span class="keyword">from</span> logging <span class="keyword">import</span> getLogger</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SeleniumMiddleware</span>():</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, timeout=<span class="literal">None</span>, service_args=[]</span>):</span></span><br><span class="line">      self.logger = getLogger(__name__)</span><br><span class="line">      self.timeout = timeout</span><br><span class="line">      self.browser = webdriver.PhantomJS(service_args=service_args)</span><br><span class="line">      self.browser.set_window_size(<span class="number">1400</span>, <span class="number">700</span>)</span><br><span class="line">      self.browser.set_page_load_timeout(self.timeout)</span><br><span class="line">      self.wait = WebDriverWait(self.browser, self.timeout)</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__del__</span>(<span class="params">self</span>):</span></span><br><span class="line">      self.browser.close()</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">process_request</span>(<span class="params">self, request, spider</span>):</span></span><br><span class="line">      <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">      用PhantomJS抓取页面</span></span><br><span class="line"><span class="string">      :param request: Request对象</span></span><br><span class="line"><span class="string">      :param spider: Spider对象</span></span><br><span class="line"><span class="string">      :return: HtmlResponse</span></span><br><span class="line"><span class="string">      &quot;&quot;&quot;</span></span><br><span class="line">      self.logger.debug(<span class="string">&#x27;PhantomJS is Starting&#x27;</span>)</span><br><span class="line">      page = request.meta.get(<span class="string">&#x27;page&#x27;</span>, <span class="number">1</span>)</span><br><span class="line">      <span class="keyword">try</span>:</span><br><span class="line">          self.browser.get(request.url)</span><br><span class="line">          <span class="keyword">if</span> page &gt; <span class="number">1</span>:</span><br><span class="line">              <span class="built_in">input</span> = self.wait.until(</span><br><span class="line">                  EC.presence_of_element_located((By.CSS_SELECTOR, <span class="string">&#x27;#mainsrp-pager div.form &gt; input&#x27;</span>)))</span><br><span class="line">              submit = self.wait.until(</span><br><span class="line">                  EC.element_to_be_clickable((By.CSS_SELECTOR, <span class="string">&#x27;#mainsrp-pager div.form &gt; span.btn.J_Submit&#x27;</span>)))</span><br><span class="line">              <span class="built_in">input</span>.clear()</span><br><span class="line">              <span class="built_in">input</span>.send_keys(page)</span><br><span class="line">              submit.click()</span><br><span class="line">          self.wait.until(</span><br><span class="line">              EC.text_to_be_present_in_element((By.CSS_SELECTOR, <span class="string">&#x27;#mainsrp-pager li.item.active &gt; span&#x27;</span>), <span class="built_in">str</span>(page)))</span><br><span class="line">          self.wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, <span class="string">&#x27;.m-itemlist .items .item&#x27;</span>)))</span><br><span class="line">          <span class="keyword">return</span> HtmlResponse(url=request.url, body=self.browser.page_source, request=request, encoding=<span class="string">&#x27;utf-8&#x27;</span>,</span><br><span class="line">                              status=<span class="number">200</span>)</span><br><span class="line">      <span class="keyword">except</span> TimeoutException:</span><br><span class="line">          <span class="keyword">return</span> HtmlResponse(url=request.url, status=<span class="number">500</span>, request=request)</span><br><span class="line"></span><br><span class="line"><span class="meta">  @classmethod</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">from_crawler</span>(<span class="params">cls, crawler</span>):</span></span><br><span class="line">      <span class="keyword">return</span> cls(timeout=crawler.settings.get(<span class="string">&#x27;SELENIUM_TIMEOUT&#x27;</span>),</span><br><span class="line">                 service_args=crawler.settings.get(<span class="string">&#x27;PHANTOMJS_SERVICE_ARGS&#x27;</span>))</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>process_request() 方法中，通过 Request 的 meta 属性获取当前需要爬取的页码，调用 PhantomJS 对象的 get() 方法访问 Request 对应的 URL，相当于从 Request 对象里获取请求链接，然后再用 PhantomJS 加载，而不再用 Scrapy 里的 Downloader</p>
<h5 id="解析页面"><a href="#解析页面" class="headerlink" title="解析页面"></a>解析页面</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse</span>(<span class="params">self, response</span>):</span></span><br><span class="line">	products = response.xpath(..)</span><br><span class="line">	...</span><br></pre></td></tr></table></figure>

<p>使用 XPath 进行解析，调用 response 变量的 xpath() 方法</p>
<h4 id="13-9-Scrapy-对接-Splash"><a href="#13-9-Scrapy-对接-Splash" class="headerlink" title="13.9 Scrapy 对接 Splash"></a>13.9 Scrapy 对接 Splash</h4><p>Splash 和 Scrapy 都支持异步处理</p>
<p>在 Selenium 的对接过程中，每个页面的渲染下载是在 Downloader Middleware 里完成的，所以整个过程是阻塞式的。Scrapy 会等待这个过程完成后再继续处理和调度其它请求，影响了爬取效率，因此使用 Splash 的爬取效率币 Selenium 高很多</p>
<h4 id="13-10-通用爬虫"><a href="#13-10-通用爬虫" class="headerlink" title="13.10 通用爬虫"></a>13.10 通用爬虫</h4><h5 id="CrawSpider"><a href="#CrawSpider" class="headerlink" title="CrawSpider"></a>CrawSpider</h5><p>官方文档 <a target="_blank" rel="noopener" href="https://docs.scrapy.org/en/latest/topics/spiders.html#crawlspider">https://docs.scrapy.org/en/latest/topics/spiders.html#crawlspider</a></p>
<p>Scrapy 提供的一个通用爬虫，在 Spider 里，可以指定一些爬取规则来实现页面的提取，这些爬取规则由一个专门 的数据结构 Rule 表示，Rule 里包含提取和跟进页面的配置，Spider 会根据 Rule 来确定当前页面中的哪些链接需要继续爬取、哪些页面的爬取结果需要用哪个方法解析</p>
<p>CrawlSpider 类继承自 Spider 类</p>
<p>rules：爬取规则属性，是包含一个或多个 Rule 对象的列表。每个 Rule 对爬取网站的动作都做了定义，CrawlSpider 会读取 rules 的每一个 Rule 并进行解析</p>
<p>parse_start_url()：是一个可重写的方法，当 start_urls 里对应的 Request 得到 Response 时，该方法被调用，会解析 Response 并必须返回 Item 对象或者 Request 对象</p>
<h6 id="Rule"><a href="#Rule" class="headerlink" title="Rule"></a>Rule</h6><p>定义页面的爬取逻辑</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">scrapy</span>.<span class="title">contrib</span>.<span class="title">spiders</span>.<span class="title">Rule</span>(<span class="params">link_extractor, callback=<span class="literal">None</span>, cb_kwargs=<span class="literal">None</span>, follow=<span class="literal">None</span>, process_links=<span class="literal">None</span>, process_request=<span class="literal">None</span></span>)</span></span><br></pre></td></tr></table></figure>

<p>link_extractor：LinkExtractor 对象，通过它，Spider 可以知道从爬取的页面中提取哪些链接，提取的链接会自动生成 Request，一般常用 LxmlLinkExtractor 对象作为参数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LxmlLinkExtractor</span>(<span class="params">FilteringLinkExtractor</span>):</span></span><br><span class="line">  	<span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">  	allow 正则表达式或正则表达式列表，定义从当前页面提取出的链接哪些是符合要求的，只有符合要求的链接才会被跟进；deny 则相反</span></span><br><span class="line"><span class="string">  	allow_domains 定义了符合要求的域名，在此域名的链接才会被跟进生成新的Request；deny_domains相反</span></span><br><span class="line"><span class="string">  	restrict_xpaths 从当前页面中XPath匹配的区域提取链接，值是XPath表达式或XPath表达式列表</span></span><br><span class="line"><span class="string">  	restrict_css 从当前页面中CSS选择器匹配的区域提取链接，值是CSS选择器或CSS选择器列表</span></span><br><span class="line"><span class="string">  	&#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params"></span></span></span><br><span class="line"><span class="function"><span class="params">        self,</span></span></span><br><span class="line"><span class="function"><span class="params">        allow=(<span class="params"></span>),</span></span></span><br><span class="line"><span class="function"><span class="params">        deny=(<span class="params"></span>),</span></span></span><br><span class="line"><span class="function"><span class="params">        allow_domains=(<span class="params"></span>),</span></span></span><br><span class="line"><span class="function"><span class="params">        deny_domains=(<span class="params"></span>),</span></span></span><br><span class="line"><span class="function"><span class="params">        restrict_xpaths=(<span class="params"></span>),</span></span></span><br><span class="line"><span class="function"><span class="params">        tags=(<span class="params"><span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;area&#x27;</span></span>),</span></span></span><br><span class="line"><span class="function"><span class="params">        attrs=(<span class="params"><span class="string">&#x27;href&#x27;</span>,</span>),</span></span></span><br><span class="line"><span class="function"><span class="params">        canonicalize=<span class="literal">False</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">        unique=<span class="literal">True</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">        process_value=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">        deny_extensions=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">        restrict_css=(<span class="params"></span>),</span></span></span><br><span class="line"><span class="function"><span class="params">        strip=<span class="literal">True</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">        restrict_text=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">    </span>):</span></span><br></pre></td></tr></table></figure>

<p>callback：回调函数，每次从 link_extractor 中获取到链接时，该函数会被调用</p>
<p>避免使用 parse() 作为回调函数，由于 CrawlSpider 使用 parse() 方法来实现逻辑，如果 parse() 方法被覆盖了，CrawlSpider 将会运行失败</p>
<p>cb_kwargs：字典，它包含传递给回调函数的参数</p>
<p>follow：True 或 False，指定跟进规则从 Response 提取的链接是否需要跟进，如果 callback 为 None，follow 默认设置为 True，否则默认为 False</p>
<p>True，下一页的页面如果请求成功了就需要继续像上一个一样分析，代表继续跟进匹配分析</p>
<p>。。。</p>
<h6 id="Item-Loader"><a href="#Item-Loader" class="headerlink" title="Item Loader"></a>Item Loader</h6><p>提供一种便捷的机制帮助我们方便地提取 Item</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">scrapy</span>.<span class="title">loader</span>.<span class="title">ItemLoader</span>(<span class="params">[item, selector, response,] **kwargs</span>)</span></span><br><span class="line"><span class="class">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="class"><span class="title">item</span>:</span> Item对象，可以调用add_xpath()、add_css()、add_value()等方法来填充 Item对象</span><br><span class="line">selector: 是Selector对象，用来提取填充数据的选择器</span><br><span class="line">response: 是Response对象，用来使用构造选择器的Response</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>

<p>一个比较典型的Item Loader</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scrapy.loader <span class="keyword">import</span> ItemLoader</span><br><span class="line"><span class="keyword">from</span> project.items <span class="keyword">import</span> Product</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse</span>(<span class="params">self, response</span>):</span></span><br><span class="line">loader = ItemLoader(item=Product(), response=response)</span><br><span class="line">loader.add_xpath(<span class="string">&#x27;name&#x27;</span>, <span class="string">&#x27;//div[@class=&quot;procuct_name&quot;]&#x27;</span>)</span><br><span class="line">loader.add_css(<span class="string">&#x27;stock&#x27;</span>, <span class="string">&#x27;p#stock&#x27;</span>)</span><br><span class="line">loader.add_value(<span class="string">&#x27;last_update&#x27;</span>, <span class="string">&#x27;today&#x27;</span>)</span><br><span class="line"><span class="keyword">return</span> loader.load_item()</span><br></pre></td></tr></table></figure>

<p>Item Loader 每个字段中都包含了一个 Input Processor(输入处理器) 和 一个 Output Processor(输出处理器)</p>
<p>InputProcessor 收到数据时立刻提取数据，保存到 ItemLoader 内，但不会分配给Item load_item 调用来生成 Item 对象。调用时会先调用  Output Processor 来处理之前收集到的数据，然后在存入 Item 中</p>
<p>一些内置 Processor</p>
<ul>
<li>Identity</li>
</ul>
<p>最简单的 Processor，不进行任何处理，直接返回原来的数据</p>
<ul>
<li>TakeFirst</li>
</ul>
<p>返回列表的第一个非空值，类似 extract_firs() 的功能，常用作 Output Processor</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scrapy.loader.processor <span class="keyword">import</span> TakeFirst</span><br><span class="line">processor = TakeFirst()</span><br><span class="line">print([<span class="string">&#x27;&#x27;</span>,<span class="string">&#x27;1&#x27;</span>,<span class="string">&#x27;2&#x27;</span>]) </span><br><span class="line"><span class="comment">#结果：输出 1</span></span><br></pre></td></tr></table></figure>

<ul>
<li>Join</li>
</ul>
<p>相当于字符串的 join()方法，可以把列表拼合成字符串，字符串默认使用空格分隔</p>
<p>也可以通过参数更改默认分隔符</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scrapy.loader.processor <span class="keyword">import</span> Join</span><br><span class="line">processor = Join()</span><br><span class="line">print(processor([<span class="string">&#x27;one&#x27;</span>, <span class="string">&#x27;two&#x27;</span>, <span class="string">&#x27;three&#x27;</span>]))</span><br><span class="line"><span class="comment">#结果： one two three</span></span><br><span class="line">processor = Join(<span class="string">&#x27;,&#x27;</span>)</span><br><span class="line"><span class="comment">#结果： one,two,three</span></span><br></pre></td></tr></table></figure>

<ul>
<li>Compose</li>
</ul>
<p>是用给定的多个函数的组合而构造的 Processor，每个输入值被传递到第一个函数，其输出再传递到第二个函数，依次类推，知道最后一个函数返回整个处理器的输出</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scrapy.loader.processor <span class="keyword">import</span> Compose</span><br><span class="line">processor = Compose(<span class="built_in">str</span>.upper, <span class="keyword">lambda</span> s: s.strip())</span><br><span class="line">print(processor(<span class="string">&#x27; hello world&#x27;</span>))</span><br><span class="line"><span class="comment">#结果：HELLO WORLD</span></span><br></pre></td></tr></table></figure>

<p>第一个参数 str.upper，第二个是一个匿名函数，strip() 方法去除头尾空白字符，Compose 会顺次调用两个参数</p>
<ul>
<li>MapCompose</li>
</ul>
<p>与 Compose 类似，MapCompose可以迭代处理一个列表输入值</p>
<p>被处理对象是一个可迭代对象</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scrapy.loader.processor <span class="keyword">import</span> MapCompose</span><br><span class="line">processor = MapCompose(<span class="built_in">str</span>.upper, <span class="keyword">lambda</span> s: s.strip())</span><br><span class="line">print(processor([<span class="string">&#x27;Hello&#x27;</span>, <span class="string">&#x27;world&#x27;</span>, <span class="string">&#x27;Python&#x27;</span>]))</span><br><span class="line"><span class="comment">#结果：[&#x27;HELLO&#x27;, &#x27;WORLD&#x27;, &#x27;PYTHON&#x27;]</span></span><br></pre></td></tr></table></figure>

<ul>
<li>SelectJmes</li>
</ul>
<p>SelectJmes 可以查询 JSON，传入 Key，返回查询所得到的 Value，要安装 jmespath 库</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip3 install jmespath</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scrapy.loader.processor <span class="keyword">import</span> SelectJmes</span><br><span class="line">processor = SeleJme(<span class="string">&#x27;foo&#x27;</span>)</span><br><span class="line">print(processor(&#123;<span class="string">&#x27;foo&#x27;</span>: <span class="string">&#x27;bar&#x27;</span>&#125;))</span><br><span class="line"><span class="comment">#结果：bar</span></span><br></pre></td></tr></table></figure>





<h5 id="爬取中华网科技类新闻"><a href="#爬取中华网科技类新闻" class="headerlink" title="爬取中华网科技类新闻"></a>爬取中华网科技类新闻</h5><p><a target="_blank" rel="noopener" href="https://tech.china.com/articles/">https://tech.china.com/articles/</a> 爬取新闻列表中所有分页的新闻详情，包括标题、正文、时间、来源</p>
<h6 id="新建项目"><a href="#新建项目" class="headerlink" title="新建项目"></a>新建项目</h6><p>新建 Scrapy 项目，名为 scrapyuniversal</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy startproject scrapyuniversal</span><br></pre></td></tr></table></figure>

<p>创建一个 CrawlSpider，需要先指定一个模板，看看哪些模板可用</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">scrapy genspider -l</span><br><span class="line">#运行结果</span><br><span class="line">Available templates:</span><br><span class="line">  basic</span><br><span class="line">  crawl</span><br><span class="line">  csvfeed</span><br><span class="line">  xmlfeed</span><br></pre></td></tr></table></figure>

<p>创建 Spider 的时候默认使用了第一个模板 basic，要创建 CrawlSpider 选择 crawl 模板</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy genspider -t crawl china tech.china.com</span><br></pre></td></tr></table></figure>

<p>生成的 Spider 内容多了一个 rules，回调函数不再是 parse，而是 parse_item</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">rules = (</span><br><span class="line">    Rule(LinkExtractor(allow=<span class="string">r&#x27;Items/&#x27;</span>), callback=<span class="string">&#x27;parse_item&#x27;</span>, follow=<span class="literal">True</span>),</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<h6 id="定义-Rule"><a href="#定义-Rule" class="headerlink" title="定义 Rule"></a>定义 Rule</h6><p>要实现新闻的爬取，需要做的就是定义好 Rule，然后实现解析函数</p>
<p>修改 start_urls </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">start_urls = [<span class="string">&#x27;http://tech.china.com/articles/&#x27;</span>]</span><br></pre></td></tr></table></figure>

<p>Spider 爬取 start_urls 里的每一个链接，得到 Response 之后，Spider 就会根据 Rule 里来提取这个页面内的超链接，去生成进一步的 Request，接下来定义 Rule 来指定提取哪些链接</p>
<p>将新闻列表中每条新闻的详情链接提取出来</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">&quot;wntjItem item_defaultView clearfix&quot;</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">&quot;item_con&quot;</span> <span class="attr">style</span>=<span class="string">&quot;margin-left: 0px;&quot;</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">&quot;item-con-inner&quot;</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">h3</span> <span class="attr">class</span>=<span class="string">&quot;tit&quot;</span>&gt;</span><span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">&quot;http://tech.china.com/article/20220224/202202241016344.html&quot;</span> <span class="attr">target</span>=<span class="string">&quot;_blank&quot;</span>&gt;</span>世界首台量子重力仪“走出实验室”，对学界、业界和国家安全等将具有深远影响<span class="tag">&lt;/<span class="name">a</span>&gt;</span><span class="tag">&lt;/<span class="name">h3</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">&quot;item_foot&quot;</span>&gt;</span> <span class="tag">&lt;<span class="name">span</span> <span class="attr">class</span>=<span class="string">&quot;time&quot;</span>&gt;</span>2022-02-24 05:59<span class="tag">&lt;/<span class="name">span</span>&gt;</span> <span class="tag">&lt;<span class="name">span</span> <span class="attr">class</span>=<span class="string">&quot;tag&quot;</span>&gt;</span><span class="tag">&lt;/<span class="name">span</span>&gt;</span> <span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">&quot;item_num&quot;</span>&gt;</span><span class="tag">&lt;<span class="name">i</span> <span class="attr">class</span>=<span class="string">&quot;s-nub&quot;</span>&gt;</span><span class="tag">&lt;/<span class="name">i</span>&gt;</span><span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>可以用 LinkExtractor 的 restrict_xpaths 属性来指定，之后 Spider 就会从这个区域提取所有超链接生成 Request。</p>
<p>但是每篇文章的导航中可能还有一些其它超链接标签，我们只需要新闻的超链接，真正新闻超链接路径都是以article开头的，用一个正则表达式将其匹配出来再赋值给 allow 即可，还需要指定一个回调函数 callback</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">rules = (</span><br><span class="line">  Rule(LinkExtractor(allow=<span class="string">&#x27;article\/.*\.html&#x27;</span>, restrict_xpaths=<span class="string">&#x27;//[@id=&quot;rank-defList&quot;]//[@class=&quot;tit&quot;]&#x27;</span>), callback=<span class="string">&#x27;parse_item&#x27;</span>, follow=<span class="literal">True</span>),</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<p>还要让当前页面实现分页功能，所以还需要提取下一页的链接</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">&quot;pages&quot;</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">ul</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">a</span> <span class="attr">class</span>=<span class="string">&quot;a1&quot;</span>&gt;</span>12219条<span class="tag">&lt;/<span class="name">a</span>&gt;</span> </span><br><span class="line">    <span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">&quot;http://tech.china.com/articles/index.html&quot;</span> <span class="attr">class</span>=<span class="string">&quot;a1&quot;</span>&gt;</span>上一页<span class="tag">&lt;/<span class="name">a</span>&gt;</span> <span class="tag">&lt;<span class="name">span</span>&gt;</span>1<span class="tag">&lt;/<span class="name">span</span>&gt;</span> </span><br><span class="line">	<span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">&quot;http://tech.china.com/articles/index_2.html&quot;</span>&gt;</span>2<span class="tag">&lt;/<span class="name">a</span>&gt;</span> </span><br><span class="line">  <span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">&quot;http://tech.china.com/articles/index_9.html&quot;</span>&gt;</span>9<span class="tag">&lt;/<span class="name">a</span>&gt;</span> </span><br><span class="line">  <span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">&quot;http://tech.china.com/articles/index_10.html&quot;</span>&gt;</span>10<span class="tag">&lt;/<span class="name">a</span>&gt;</span> ..</span><br><span class="line">  <span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">&quot;http://tech.china.com/articles/index_489.html&quot;</span>&gt;</span>489<span class="tag">&lt;/<span class="name">a</span>&gt;</span> </span><br><span class="line">  <span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">&quot;http://tech.china.com/articles/index_2.html&quot;</span> <span class="attr">class</span>=<span class="string">&quot;a1&quot;</span>&gt;</span>下一页<span class="tag">&lt;/<span class="name">a</span>&gt;</span>	</span><br><span class="line">  <span class="tag">&lt;/<span class="name">ul</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>再加一个 Rule</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Rule(LinkExtractor(restrict_xpaths=<span class="string">&#x27;//div[@class=&quot;pages&quot;]//a[contains(., &quot;下一页&quot;)]&#x27;</span>))</span><br></pre></td></tr></table></figure>

<p>不需要像新闻详情页一样去提取页面详细信息，也就是不需要生成 Item，所以不需要加 callback 参数</p>
<p>接着运行代码</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy crawl china</span><br></pre></td></tr></table></figure>

<p>查看输出，实现了翻页和详情页的抓取了</p>
<h6 id="解析页面-1"><a href="#解析页面-1" class="headerlink" title="解析页面"></a>解析页面</h6><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_item</span>(<span class="params">self, response</span>):</span></span><br><span class="line">    loader = ChinaLoader(item=NewsItem(), response=response)</span><br><span class="line">    loader.add_xpath(<span class="string">&#x27;title&#x27;</span>, <span class="string">&#x27;//h1[@id=&quot;chan_newsTitle&quot;]/text()&#x27;</span>)</span><br><span class="line">    loader.add_value(<span class="string">&#x27;url&#x27;</span>, response.url)</span><br><span class="line">    loader.add_xpath(<span class="string">&#x27;text&#x27;</span>, <span class="string">&#x27;//div[@id=&quot;chan_newsDetail&quot;]/text()&#x27;</span>)</span><br><span class="line">    loader.add_xpath(<span class="string">&#x27;datetime&#x27;</span>, <span class="string">&#x27;//div[@id=&quot;chan_newsInfo&quot;]//*[@class=&quot;time&quot;]/text()&#x27;</span>,re=<span class="string">&#x27;(\d+-\d+-\d+\s\d+:\d+:\d+)&#x27;</span>)</span><br><span class="line">    loader.add_xpath(<span class="string">&#x27;source&#x27;</span>, <span class="string">&#x27;//div[@id=&quot;chan_newsInfo&quot;]//*[@class=&quot;source&quot;]/text()&#x27;</span>, re=<span class="string">&#x27;来源：(.*)&#x27;</span>)</span><br><span class="line">    loader.add_value(<span class="string">&#x27;website&#x27;</span>, <span class="string">&#x27;中华网&#x27;</span>)</span><br><span class="line">    <span class="keyword">yield</span> loader.load_item()</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scrapy.loader <span class="keyword">import</span> ItemLoader</span><br><span class="line"><span class="keyword">from</span> scrapy.loader.processors <span class="keyword">import</span> TakeFirst, Join, Compose</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">NewsLoader</span>(<span class="params">ItemLoader</span>):</span></span><br><span class="line">  default_output_processor = TakeFirst()</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ChinaLoader</span>(<span class="params">NewsLoader</span>):</span></span><br><span class="line">  text_out = Compose(Join(), <span class="keyword">lambda</span> s: s.strip()) <span class="comment">#把列表拼合成一个字符串</span></span><br><span class="line">  source_out = Compose(Join(), <span class="keyword">lambda</span> s: s.strip())</span><br></pre></td></tr></table></figure>

<h6 id="通用配置抽取"><a href="#通用配置抽取" class="headerlink" title="通用配置抽取"></a>通用配置抽取</h6><h4 id="13-11-Scrapyt"><a href="#13-11-Scrapyt" class="headerlink" title="13.11 Scrapyt"></a>13.11 Scrapyt</h4><p>Scrapyt 为 Scrapy 提供了一个调度的 HTTP 接口，有了它就不需要再执行 Scrapy 命令，而是通过请求一个 HTTP 接口即可调度 Scrapy 任务，如果项目是在远程服务器运行，可以利用它来启动项目</p>
<h5 id="GET-请求"><a href="#GET-请求" class="headerlink" title="GET 请求"></a>GET 请求</h5><p>支持如下参数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">spider_name：Spider 名称</span></span><br><span class="line"><span class="string">url：爬取链接，如果传递了该参数，Scrapy会直接使用该URL生成Request，而直接忽略start_requests()方法和start_urls属性的定义</span></span><br><span class="line"><span class="string">callback：回调函数名称</span></span><br><span class="line"><span class="string">max_requests：最大请求数量，可选，如定义5，表示最多执行5次Request请求，其余会被忽略</span></span><br><span class="line"><span class="string">start_requests：是否要执行start_requests方法，布尔类型，可选，Scrapy项目中如果定义了start_requests方法，项目启动时会默认调用该方法，Scrapyrt中就不一样，默认不执行start_requests方法，如果要执行start_requests，设置为true</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>

<p>在项目目录下运行 Scrapyrt，默认服务运行在 9080 端口上</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl http:&#x2F;&#x2F;localhost:9080&#x2F;crawl.json\?spider_name\&#x3D;china\&amp;url\&#x3D;http:&#x2F;&#x2F;tech.china.com&#x2F;articles&#x2F;</span><br></pre></td></tr></table></figure>

<p>会返回一个 JSON 格式，status 显示了爬取状态，items 部分是项目的爬取结果，stats 是爬取的统计情况，items_dropped 是被忽略的 Item 列表</p>
<h5 id="POST-请求"><a href="#POST-请求" class="headerlink" title="POST 请求"></a>POST 请求</h5><p>Request Body 必须是一个合法的JSON配置，JSON里面可以配置响应的参数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">spider_name</span></span><br><span class="line"><span class="string">max_requests</span></span><br><span class="line"><span class="string">request: Request配置，JSON对象，必传参数，通过该参数可以定义Request的各个参数，必须指定url字段指定爬取链接,其它参数可选</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>

<p>JSON配置实例</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">	<span class="attr">&quot;request&quot;</span>:&#123;</span><br><span class="line">    <span class="attr">&quot;url&quot;</span>: <span class="string">&quot;http://quotes.toscrape.com/&quot;</span>,</span><br><span class="line">    <span class="attr">&quot;callback&quot;</span>: <span class="string">&quot;parse&quot;</span>,</span><br><span class="line">    <span class="attr">&quot;dont_filter&quot;</span>: <span class="string">&quot;True&quot;</span>,</span><br><span class="line">    <span class="attr">&quot;cookies&quot;</span>:&#123;</span><br><span class="line">      <span class="attr">&quot;foo&quot;</span>: <span class="string">&quot;bar&quot;</span>,</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="attr">&quot;max_requests&quot;</span>:<span class="number">2</span>,</span><br><span class="line">  <span class="attr">&quot;spider_name&quot;</span>:quotes</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>执行下面命令传递该JSON配置并发起POST请求</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl http:&#x2F;&#x2F;localhost:9080&#x2F;crawl.json -d &#39;&#123;&quot;requests&quot;...JSON字符串&#125;&#39;</span><br></pre></td></tr></table></figure>

<h4 id="Scrapy对接Docker"><a href="#Scrapy对接Docker" class="headerlink" title="Scrapy对接Docker"></a>Scrapy对接Docker</h4><p>把 Scrapy 项目制作成一个 Docker 镜像，只要其它主机安装了 Docker，那么只要将镜像下载并运行即可，而不必担心环境问题或版本问题</p>
<h5 id="创建Dockerfile"><a href="#创建Dockerfile" class="headerlink" title="创建Dockerfile"></a>创建Dockerfile</h5><p>项目根目录下新建 requirements.txt，将整个项目依赖的 Python 环境包都列出来，如</p>
<p>如果库需要特定的版本，还可以指定版本号</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">scrapy</span><br><span class="line">pymongo</span><br><span class="line">scrapy&gt;&#x3D;1.4.0</span><br><span class="line">pymongo&gt;&#x3D;3.4.0</span><br></pre></td></tr></table></figure>

<p>根目录下新建 Dockerfile 没有后缀，修改内容</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">FROM python:3.8 </span><br><span class="line">ENV PATH &#x2F;usr&#x2F;local&#x2F;bin:$PATH</span><br><span class="line">ADD . &#x2F;code</span><br><span class="line">WORKDIR &#x2F;code</span><br><span class="line">RUN pip3.8 install -r requirements.txt</span><br><span class="line">CMD scrapy crawl china</span><br><span class="line">#第一行From代表使用 Docker 基础镜像，这里直接使用 python:3.8的镜像，在此基础上运行项目</span><br><span class="line">#第二行ENV是环境变量设置，增加&#x2F;usr&#x2F;local&#x2F;bin这个环境变量路径</span><br><span class="line">#第三行ADD是将本地的代码放置到虚拟容器中 .本地当前路径 &#x2F;code代表虚拟容器中的路径</span><br><span class="line">#第四行WORKDIR是指定工作目录，这里将刚添加的代码路径设置成工作路径</span><br><span class="line">#第五行RUN是执行某些命令来做一些环境准备工作</span><br><span class="line">#第六行CMD是容器启动命令，容器运行时，此命令会被执行</span><br></pre></td></tr></table></figure>

<h5 id="构建镜像"><a href="#构建镜像" class="headerlink" title="构建镜像"></a>构建镜像</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker build -t china:latest .</span><br></pre></td></tr></table></figure>

<p>构建成功后，查看镜像</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">docker images</span><br><span class="line">#china latest 41c797934ce 2 minutes ago 768M</span><br></pre></td></tr></table></figure>

<h5 id="运行-1"><a href="#运行-1" class="headerlink" title="运行"></a>运行</h5><p>镜像可以本地测试运行，这样就利用此镜像新建并运行了一个 Docker 容器，运行效果完全一致</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run china</span><br></pre></td></tr></table></figure>

<h5 id="推送至-Docker-Hub"><a href="#推送至-Docker-Hub" class="headerlink" title="推送至 Docker Hub"></a>推送至 Docker Hub</h5><p>构建完成后，可以将镜像 Push 到 Docker 镜像托管平台，Docker Hub 或者私有的 Docker Registry 等，这样就可以从远程服务器下载镜像并运行了</p>
<p><a target="_blank" rel="noopener" href="https://hub.docker.com/">https://hub.docker.com</a> 注册账号，新建一个 Reponsitory，名为 quotes，比如用户名为 germey，那么此 Reponsitory 的地址就可以用 germey/quotes 来表示</p>
<p>打标签，推送镜像到 Docker Hub</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">docker tag quotes:latest germey&#x2F;quotes:latest</span><br><span class="line">docker push germey&#x2F;quotes</span><br></pre></td></tr></table></figure>

<p>如果我们想在其他的主机上运行这个镜像，主机上装好 Docker 后，运行命令</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run germey&#x2F;quotes</span><br></pre></td></tr></table></figure>

<p>会自动下载镜像，启动容器运行，不需要配置 Python 环境，不需要关系版本冲突的问题</p>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/Python/" rel="tag"># Python</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2022/04/19/Python3%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%E5%BC%80%E5%8F%91%E5%AE%9E%E6%88%98-pyspider%E6%A1%86%E6%9E%B6%E4%BD%BF%E7%94%A8/" rel="prev" title="Python3网络爬虫开发实战-pyspider框架使用">
                  <i class="fa fa-chevron-left"></i> Python3网络爬虫开发实战-pyspider框架使用
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2022/04/21/Python3%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%E5%BC%80%E5%8F%91%E5%AE%9E%E6%88%98-%E4%BB%A3%E7%90%86%E7%9A%84%E4%BD%BF%E7%94%A8/" rel="next" title="Python3网络爬虫开发实战-代理的使用">
                  Python3网络爬虫开发实战-代理的使用 <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






      

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      const activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      const commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

    </div>
  </main>

  <footer class="footer">
    <div class="footer-inner">
      

      

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">daxun</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/muse/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <script src="//cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js"></script>
<script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script>

  




  <script src="/js/local-search.js"></script>















  








  

  

</body>
</html>
